DONE

	Implement basic throughput test in google cloud. How far can I push it?

	Seems that we can do around 3 * 1000 * 1000 = 3m packets per-second, @ 256 bytes average packet size.

	We seem to overload around 0.7 gigabytes per-second. This is reasonable enough for now.

	The limit seems to be more packets per-second, rather than bandwidth. Greater throughput can be obtained, with larger packet sizes.

	Upgrade client to use atomics.

	Extend packets sent to proxy to include a uint64 sequence number in the first 8 bytes.

	Track packet loss via packet buffer, and print it out.

	Now run the client and proxy in the cloud and verify we get zero packet loss at a baseline load.

	Confirmed. Works with two clients, starts dropping packets with a third.

	What's interesting is that if the proxy gets overloaded, reducing the load doesn't seem to fix it.

	Clients get dropped packets until they are all reset. Seems weird. Shouldn't the dropped packets stop once load is reduced? Does the proxy get into a pathological oveloaded state?

TODO

	Next step is to implement an actual server, and make the proxy "real" (eg. it always uploads packets for a client to a specific server).

	eg.

	client <-> proxy <-> server.

	---------------

















-------------------------

	Create a bootstrap script for the client, so it can be setup as a mig and scaled up/down easily.

	Explore alternative ways to receive and send packets, eg. sendmmsg, recvmmsg in Linux, to potentially reduce overhead.
